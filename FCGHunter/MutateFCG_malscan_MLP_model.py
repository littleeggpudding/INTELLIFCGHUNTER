import sys
import os
import pygmo as pg
import tensorflow as tf
import torch
import argparse
import random
import copy
import psutil
import glob
import pickle


sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))
from type.FCG import FCG
import numpy as np
from utils import load_model, load_MLP_model
from utils import save_graph_for_every_generation, save_log_for_every_generation, save_log_for_every_individual
from utils import _init_population_our as _init_population
from utils import deal_with_conflict
from utils import calculate_opposite_adjustment
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("device", device)


#global variable
save_dir = None
target_model = None #target model, MLP
feature_type = None


def ga(fcg, max_generation=100, pop_num=100, steps=300, sensitive=True, group=False):
    """
        Genetic Algorithm (GA) for optimizing the function call graph (FCG) through mutation and crossover operations.
        Key steps:
        1. **Initialization**: A population of FCG mutations is created with `_init_population`.
        2. **Fitness Evaluation**: The population is scored using `_population_score` to evaluate how well each individual performs.
        3. **Selection**: The two best-performing individuals are selected using `_ga_select` for crossover and mutation.
        4. **Crossover & Mutation**: New individuals are generated by crossing over two selected parents and applying mutation steps to further evolve them.
        5. **Conflict Resolution**: Any conflicts within the mutated FCGs are resolved using `deal_with_conflict`.
        6. **Evolution**: The top-performing individuals from the current and previous generations are selected to form the next generation.
        7. **Termination**: The process repeats for `max_generation` iterations, or stops early if an optimal solution is found.
    """
    # sensitive: for control if the mutation is in sensitive area
    # group: for control if using dependency-aware

    # 1. init population
    population = _init_population(fcg, pop_num, steps, sensitive, group)
    pop_score_list, pop_shap_list = _population_score(fcg, population)

    indexes = np.argsort(pop_score_list)[:pop_num]
    population = [population[i] for i in indexes]
    pop_score_list = [pop_score_list[i] for i in indexes]
    pop_shap_list = [pop_shap_list[i] for i in indexes]

    if len(pop_score_list) == 0:
        return None

    # 2. ga
    for i in range(max_generation):
        tf.keras.backend.clear_session()

        # 2.1 judge if the target is found
        target_index = _fitness_function(pop_score_list)
        if target_index != -1:
            print("fitness function is true")
            save_log_for_every_individual(fcg, population[target_index], pop_score_list[target_index], i, save_dir)
            break

        next_gene_population = []
        save_log_for_every_generation(fcg, pop_score_list, pop_shap_list, i, population, save_dir)

        # 2.2 generate next generation
        while True:
            if len(next_gene_population) >= pop_num:
                next_gene_population = next_gene_population[:pop_num]
                break

            # 2.3 random select two individual, using tornament selection
            individual1_index, individual2_index = _ga_select(population, pop_score_list, pop_shap_list,
                                                              int(pop_num / 2) - 1)
            individual1 = population[individual1_index]
            individual2 = population[individual2_index]

            new_individual = copy.deepcopy(individual1)
            new_individual.crossover(individual2, fcg)
            cur_fcg = deal_with_conflict(fcg, new_individual, group)

            # 2.4 ga mutation
            new_individual.ga_mutation(cur_fcg, 0.2, sensitive)

            # 2.5 update dependency
            if not group:
                new_individual.update_dependency()

            # 2.6 append to next_gene_population
            next_gene_population.append(new_individual)

        cur_pop_score_list, cur_pop_shap_list = _population_score(fcg, next_gene_population)
        if len(cur_pop_score_list) == 0:
            return None
        combined_population = population + next_gene_population
        combined_score_list = pop_score_list + cur_pop_score_list
        combined_shap_list = pop_shap_list + cur_pop_shap_list

        top_pop, top_score, top_shap = sort_all_pop(combined_population, combined_score_list, combined_shap_list,
                                                    pop_num)
        population = top_pop
        pop_score_list = top_score
        pop_shap_list = top_shap

    return population


def _individual_score(fcg, individual):
    fcg = copy.deepcopy(fcg)

    original_combined_feature = extract_feature_use_fcg(fcg)
    if original_combined_feature is None:
        print("feature is none")
        return None

    for j in range(len(individual.final_group_list)):
        group = individual.final_group_list[j]
        safe_mutation = []
        for i in range(len(group)):
            mutation = group[i]
            state, res = fcg.process_mutation(mutation)
            if state:
                safe_mutation.append(mutation)
        individual.final_group_list[j] = safe_mutation

    # 1. features
    combined_feature = extract_feature_use_fcg(fcg)
    if combined_feature is None:
        print("new feature is none")
        return None

    return original_combined_feature, combined_feature, fcg


def extract_feature_use_fcg(fcg):
    if feature_type == 'concentrate' or feature_type == 'average':
        # combination type
        fcg.cal_centralities()

        degree = fcg.degree_feature

        if degree is None:
            return None
        katz = fcg.katz_feature

        if katz is None:
            return None

        closeness = fcg.closeness_feature
        harmonic = fcg.harmonic_feature

        combined_feature = None
        if feature_type == 'concentrate':
            combined_feature = np.concatenate((degree, katz, closeness, harmonic), axis=0)
            return combined_feature

        elif feature_type == 'average':
            combined_feature = cal_4features_avg(degree, katz, closeness, harmonic)
        return combined_feature

    else:
        # single feature
        fcg.cal_centralities(feature_type)
        combined_feature = None
        if feature_type == 'degree':
            combined_feature = fcg.degree_feature
        elif feature_type == 'katz':
            combined_feature = fcg.katz_feature
        elif feature_type == 'closeness':
            combined_feature = fcg.closeness_feature
        elif feature_type == 'harmonic':
            combined_feature = fcg.harmonic_feature

        return np.array(combined_feature)


def _fitness_function(pop_score_list):
    target_index = -1
    for i in range(len(pop_score_list)):
        if pop_score_list[i] is not None and pop_score_list[i] < 0:
            target_index = i
            break
    return target_index

def _population_score(fcg, pop, save_state = False):
    # shap+score
    score_list = []
    shap_list = []
    pop_feature_list = []
    pop_fcg_list = []

    for i in range(len(pop) - 1, -1, -1):
        res = _individual_score(fcg, pop[i])
        # if score is None or shap_sum is None:

        if res is None:
            del pop[i]
        else:
            original_combined_feature, combined_feature, cur_fcg = res
            shap_sum = calculate_opposite_adjustment(original_combined_feature, combined_feature, fcg.shap_value)
            shap_list.append(shap_sum)
            combined_feature = combined_feature.reshape(1, -1)
            pop_feature_list.extend(combined_feature)
            pop_fcg_list.append(cur_fcg)

    pop_feature_list = np.array(pop_feature_list)
    # print("pop_feature_list", pop_feature_list.shape)
    all_pop_score = target_model.predict(pop_feature_list)
    for i in range(len(all_pop_score)):
        score = all_pop_score[i][0]
        score_list.append(score)

        cur_fcg = pop_fcg_list[i]
        if score < 0:
            save_graph_for_every_generation(cur_fcg, 'success', -1, save_dir)

    return score_list[::-1], shap_list[::-1]

def select(pop_score_list, pop_shap_list, idxes=None):
    # dominate version
    pop_score_candidate = [pop_score_list[idx] for idx in idxes]
    pop_shap_candidate = [pop_shap_list[idx] for idx in idxes]

    if len(idxes) == 1:
        return idxes[0]

    # For the score, the smaller the better; for the shap, the larger the better
    com = []
    for i in range(len(pop_score_candidate)):
        score = pop_score_candidate[i]
        shap = -pop_shap_candidate[i]
        com.append([score, shap])

    ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(points=com)

    return idxes[ndf[0][0]]


def sort_all_pop(pop, pop_score_list, pop_shap_list, pop_number):
    # dominate version
    com = []
    for i in range(len(pop_score_list)):
        score = pop_score_list[i]
        shap = -pop_shap_list[i]
        com.append([score, shap])

    # print("com", com)
    ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(points=com)
    # print("ndf", ndf)
    pop_total = []
    pop_score_list_total = []
    pop_shap_list_total = []
    for level in ndf:
        for idx in level:
            # print("idx", idx)
            pop_total.append(pop[idx])
            pop_score_list_total.append(pop_score_list[idx])
            pop_shap_list_total.append(pop_shap_list[idx])
            if len(pop_total) >= pop_number:
                break

        if len(pop_total) >= pop_number:
            break

    return pop_total, pop_score_list_total, pop_shap_list_total



def _ga_select(pop, pop_score_list, pop_shap_list, tournament_size=20):
    # Ensure the tournament size does not exceed the population size
    tournament_size = min(tournament_size, len(pop))

    # Select two individuals' indices
    winner_indices = set()
    while True:
        tournament_indices = random.sample(range(len(pop)), tournament_size)
        winner_index = select(pop_score_list, pop_shap_list, tournament_indices)
        winner_indices.add(winner_index)
        if len(winner_indices) >= 2:
            break

    winner_indices = list(winner_indices)
    return winner_indices[0], winner_indices[1]

def parse_arguments():
    """
    Function to initialize the argument parser and parse command-line arguments.

    Returns:
    --------
    args : argparse.Namespace
        Parsed arguments from the command line.
    """
    # Initialize the argument parser
    parser = argparse.ArgumentParser(description="Process some input paths and settings.")

    # Define arguments
    parser.add_argument('--save_path', type=str, required=True, help="Directory to save the results.")
    parser.add_argument('--attack_sample', type=str, required=True, help="Path to the attack samples.")
    parser.add_argument('--target_model', type=str, required=True, help="Path to the target model.")
    parser.add_argument('--shap_value', type=str, help="Path to the shap value file (optional).")
    parser.add_argument('--feature_type', type=str,
                        choices=['concentrate', 'average', 'degree', 'katz', 'closeness', 'harmonic'], required=True,
                        help="Type of MalScan feature extraction.")


    # basic parameters
    parser.add_argument('--pop_num', type=int, required=True, help="Number of population during GA.")
    parser.add_argument('--max_generation', type=int, required=True, help="Number of max generations.")
    parser.add_argument('--steps', type=int, required=True, help="Number of mutation steps.")


    # optional parameters for ablation study
    parser.add_argument('--non-sensitive', action='store_true', help="Disable sensitive area.")
    parser.add_argument('--non-dependency', action='store_true', help="Disable dependency-aware.")
    parser.add_argument('--non-shap', action='store_true', help="Disable shap value.")

    return parser.parse_args()


if __name__ == '__main__':
    # Systematically set the CPU affinity for the current process
    p = psutil.Process(os.getpid())
    p.cpu_affinity(list(range(1, 110)))

    # For running the script in parallel
    if len(sys.argv) > 2:
        try:
            proc_id = int(sys.argv[1])
            proc_total = int(sys.argv[2])
            print(f"Received part index: {proc_id}")
        except ValueError:
            proc_id = 0
            proc_total = 1
            print("Error: The first argument must be an integer.")
    else:
        proc_id = 0
        proc_total = 1

    # 1.Initialize the parameters
    args = parse_arguments()

    # 2. Read attack samples from txt file
    attack_samples = glob.glob(args.attack_sample)
    print("attack_samples", len(attack_samples))

    part_size = len(attack_samples) // proc_total
    extra = len(attack_samples) % proc_total
    start_index = proc_id * part_size + min(proc_id, extra)
    if proc_id < extra:
        part_size += 1
    end_index = start_index + part_size

    # 3. Load the target model, MLP
    target_model = load_MLP_model(args.target_model)

    # 4. Load the shap value
    shap_values = None  # including all attack samples, list type
    feature_type = args.feature_type
    if args.non_shap is False:
        with open(args.shap_value, 'rb') as file:
            shap_values = pickle.load(file)
        shap_values = shap_values[0]  # only using the benign's shap value
    else:
        if feature_type == 'concentrate' or feature_type == 'average':
            shap_values = [0] * 21986 * 4
        else:
            shap_values = [0] * 21986

    sensitive = True  # default is using sensitive area
    if args.non_sensitive:
        sensitive = False

    group = False  # default is using dependency-aware
    if args.non_dependency:
        group = True

    for i in range(start_index,end_index):
        fcg_file = attack_samples[i]
        fcg = FCG(fcg_file, 1, zeros_list)

        combined_feature = None
        if feature_type == 'concentrate' or feature_type == 'average':
            fcg.cal_centralities()
            if fcg.used_sensitive_nodes is not None and len(
                    fcg.used_sensitive_nodes) != 0 and fcg.degree_feature is not None:
                degree = fcg.degree_feature
                katz = fcg.katz_feature
                closeness = fcg.closeness_feature
                harmonic = fcg.harmonic_feature

                if feature_type == 'concentrate':
                    combined_feature = np.hstack((degree, katz, closeness, harmonic))
                elif feature_type == 'average':
                    combined_feature = cal_4features_avg(degree, katz, closeness, harmonic)


        else:
            feature = fcg.cal_centralities(feature_type)
            if fcg.used_sensitive_nodes is not None and len(fcg.used_sensitive_nodes) != 0 and feature is True:
                combined_feature = None
                if feature_type == 'degree':
                    combined_feature = fcg.degree_feature
                elif feature_type == 'katz':
                    combined_feature = fcg.katz_feature
                elif feature_type == 'closeness':
                    combined_feature = fcg.closeness_feature
                elif feature_type == 'harmonic':
                    combined_feature = fcg.harmonic_feature

        if combined_feature is not None:
            new_feature = combined_feature.reshape(1, -1)

            Y_probs = target_model.predict(new_feature)
            print("Y_probs", Y_probs)

            if Y_probs[0][0] < 0:
                print("benign")

            else:
                print("malware")
                save_dir = args.save_path
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)

                generation = args.max_generation
                pop_num = args.pop_num
                steps = args.steps

                ga(fcg, generation, pop_num, steps, sensitive, group)




